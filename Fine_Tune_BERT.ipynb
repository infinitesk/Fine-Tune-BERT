{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80e7473f-9077-45fa-9bc7-7df1ac01258c",
      "metadata": {
        "id": "80e7473f-9077-45fa-9bc7-7df1ac01258c"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32b5c69-b8b2-4807-b6c5-d5ed3d237b7e",
      "metadata": {
        "id": "a32b5c69-b8b2-4807-b6c5-d5ed3d237b7e",
        "outputId": "cbcd1c0a-0ef6-46d2-ec6c-a9f6b7780dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c799a68-36aa-4ecc-8c57-2becf92b38e2",
      "metadata": {
        "id": "2c799a68-36aa-4ecc-8c57-2becf92b38e2"
      },
      "source": [
        "### load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4fbd278-3ace-49cb-9765-5dbed896d5ae",
      "metadata": {
        "id": "f4fbd278-3ace-49cb-9765-5dbed896d5ae"
      },
      "outputs": [],
      "source": [
        "dataset_dict = load_dataset(\"shawhin/phishing-site-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb6b85e-9b43-44cc-a335-e415635d7c31",
      "metadata": {
        "id": "edb6b85e-9b43-44cc-a335-e415635d7c31",
        "outputId": "1742b4c4-458a-4969-8d7f-6b12660ccd34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'labels'],\n",
              "        num_rows: 2100\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'labels'],\n",
              "        num_rows: 450\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'labels'],\n",
              "        num_rows: 450\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf82a01",
      "metadata": {
        "id": "2bf82a01"
      },
      "source": [
        "### Train Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0311bd",
      "metadata": {
        "id": "ec0311bd",
        "outputId": "97b3eb30-d605-464a-d71a-9b8e7478fea2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shawhin/opt/anaconda3/envs/model-compression-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "model_path = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "id2label = {0: \"Safe\", 1: \"Not Safe\"}\n",
        "label2id = {\"Safe\": 0, \"Not Safe\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
        "                                                           num_labels=2,\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826084b6",
      "metadata": {
        "id": "826084b6"
      },
      "source": [
        "#### Freeze base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03aa44f2",
      "metadata": {
        "id": "03aa44f2",
        "outputId": "ab05d7f8-341f-473f-9166-aa58680d994d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert.embeddings.word_embeddings.weight True\n",
            "bert.embeddings.position_embeddings.weight True\n",
            "bert.embeddings.token_type_embeddings.weight True\n",
            "bert.embeddings.LayerNorm.weight True\n",
            "bert.embeddings.LayerNorm.bias True\n",
            "bert.encoder.layer.0.attention.self.query.weight True\n",
            "bert.encoder.layer.0.attention.self.query.bias True\n",
            "bert.encoder.layer.0.attention.self.key.weight True\n",
            "bert.encoder.layer.0.attention.self.key.bias True\n",
            "bert.encoder.layer.0.attention.self.value.weight True\n",
            "bert.encoder.layer.0.attention.self.value.bias True\n",
            "bert.encoder.layer.0.attention.output.dense.weight True\n",
            "bert.encoder.layer.0.attention.output.dense.bias True\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.0.intermediate.dense.weight True\n",
            "bert.encoder.layer.0.intermediate.dense.bias True\n",
            "bert.encoder.layer.0.output.dense.weight True\n",
            "bert.encoder.layer.0.output.dense.bias True\n",
            "bert.encoder.layer.0.output.LayerNorm.weight True\n",
            "bert.encoder.layer.0.output.LayerNorm.bias True\n",
            "bert.encoder.layer.1.attention.self.query.weight True\n",
            "bert.encoder.layer.1.attention.self.query.bias True\n",
            "bert.encoder.layer.1.attention.self.key.weight True\n",
            "bert.encoder.layer.1.attention.self.key.bias True\n",
            "bert.encoder.layer.1.attention.self.value.weight True\n",
            "bert.encoder.layer.1.attention.self.value.bias True\n",
            "bert.encoder.layer.1.attention.output.dense.weight True\n",
            "bert.encoder.layer.1.attention.output.dense.bias True\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.1.intermediate.dense.weight True\n",
            "bert.encoder.layer.1.intermediate.dense.bias True\n",
            "bert.encoder.layer.1.output.dense.weight True\n",
            "bert.encoder.layer.1.output.dense.bias True\n",
            "bert.encoder.layer.1.output.LayerNorm.weight True\n",
            "bert.encoder.layer.1.output.LayerNorm.bias True\n",
            "bert.encoder.layer.2.attention.self.query.weight True\n",
            "bert.encoder.layer.2.attention.self.query.bias True\n",
            "bert.encoder.layer.2.attention.self.key.weight True\n",
            "bert.encoder.layer.2.attention.self.key.bias True\n",
            "bert.encoder.layer.2.attention.self.value.weight True\n",
            "bert.encoder.layer.2.attention.self.value.bias True\n",
            "bert.encoder.layer.2.attention.output.dense.weight True\n",
            "bert.encoder.layer.2.attention.output.dense.bias True\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.2.intermediate.dense.weight True\n",
            "bert.encoder.layer.2.intermediate.dense.bias True\n",
            "bert.encoder.layer.2.output.dense.weight True\n",
            "bert.encoder.layer.2.output.dense.bias True\n",
            "bert.encoder.layer.2.output.LayerNorm.weight True\n",
            "bert.encoder.layer.2.output.LayerNorm.bias True\n",
            "bert.encoder.layer.3.attention.self.query.weight True\n",
            "bert.encoder.layer.3.attention.self.query.bias True\n",
            "bert.encoder.layer.3.attention.self.key.weight True\n",
            "bert.encoder.layer.3.attention.self.key.bias True\n",
            "bert.encoder.layer.3.attention.self.value.weight True\n",
            "bert.encoder.layer.3.attention.self.value.bias True\n",
            "bert.encoder.layer.3.attention.output.dense.weight True\n",
            "bert.encoder.layer.3.attention.output.dense.bias True\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.3.intermediate.dense.weight True\n",
            "bert.encoder.layer.3.intermediate.dense.bias True\n",
            "bert.encoder.layer.3.output.dense.weight True\n",
            "bert.encoder.layer.3.output.dense.bias True\n",
            "bert.encoder.layer.3.output.LayerNorm.weight True\n",
            "bert.encoder.layer.3.output.LayerNorm.bias True\n",
            "bert.encoder.layer.4.attention.self.query.weight True\n",
            "bert.encoder.layer.4.attention.self.query.bias True\n",
            "bert.encoder.layer.4.attention.self.key.weight True\n",
            "bert.encoder.layer.4.attention.self.key.bias True\n",
            "bert.encoder.layer.4.attention.self.value.weight True\n",
            "bert.encoder.layer.4.attention.self.value.bias True\n",
            "bert.encoder.layer.4.attention.output.dense.weight True\n",
            "bert.encoder.layer.4.attention.output.dense.bias True\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.4.intermediate.dense.weight True\n",
            "bert.encoder.layer.4.intermediate.dense.bias True\n",
            "bert.encoder.layer.4.output.dense.weight True\n",
            "bert.encoder.layer.4.output.dense.bias True\n",
            "bert.encoder.layer.4.output.LayerNorm.weight True\n",
            "bert.encoder.layer.4.output.LayerNorm.bias True\n",
            "bert.encoder.layer.5.attention.self.query.weight True\n",
            "bert.encoder.layer.5.attention.self.query.bias True\n",
            "bert.encoder.layer.5.attention.self.key.weight True\n",
            "bert.encoder.layer.5.attention.self.key.bias True\n",
            "bert.encoder.layer.5.attention.self.value.weight True\n",
            "bert.encoder.layer.5.attention.self.value.bias True\n",
            "bert.encoder.layer.5.attention.output.dense.weight True\n",
            "bert.encoder.layer.5.attention.output.dense.bias True\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.5.intermediate.dense.weight True\n",
            "bert.encoder.layer.5.intermediate.dense.bias True\n",
            "bert.encoder.layer.5.output.dense.weight True\n",
            "bert.encoder.layer.5.output.dense.bias True\n",
            "bert.encoder.layer.5.output.LayerNorm.weight True\n",
            "bert.encoder.layer.5.output.LayerNorm.bias True\n",
            "bert.encoder.layer.6.attention.self.query.weight True\n",
            "bert.encoder.layer.6.attention.self.query.bias True\n",
            "bert.encoder.layer.6.attention.self.key.weight True\n",
            "bert.encoder.layer.6.attention.self.key.bias True\n",
            "bert.encoder.layer.6.attention.self.value.weight True\n",
            "bert.encoder.layer.6.attention.self.value.bias True\n",
            "bert.encoder.layer.6.attention.output.dense.weight True\n",
            "bert.encoder.layer.6.attention.output.dense.bias True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.6.intermediate.dense.weight True\n",
            "bert.encoder.layer.6.intermediate.dense.bias True\n",
            "bert.encoder.layer.6.output.dense.weight True\n",
            "bert.encoder.layer.6.output.dense.bias True\n",
            "bert.encoder.layer.6.output.LayerNorm.weight True\n",
            "bert.encoder.layer.6.output.LayerNorm.bias True\n",
            "bert.encoder.layer.7.attention.self.query.weight True\n",
            "bert.encoder.layer.7.attention.self.query.bias True\n",
            "bert.encoder.layer.7.attention.self.key.weight True\n",
            "bert.encoder.layer.7.attention.self.key.bias True\n",
            "bert.encoder.layer.7.attention.self.value.weight True\n",
            "bert.encoder.layer.7.attention.self.value.bias True\n",
            "bert.encoder.layer.7.attention.output.dense.weight True\n",
            "bert.encoder.layer.7.attention.output.dense.bias True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.7.intermediate.dense.weight True\n",
            "bert.encoder.layer.7.intermediate.dense.bias True\n",
            "bert.encoder.layer.7.output.dense.weight True\n",
            "bert.encoder.layer.7.output.dense.bias True\n",
            "bert.encoder.layer.7.output.LayerNorm.weight True\n",
            "bert.encoder.layer.7.output.LayerNorm.bias True\n",
            "bert.encoder.layer.8.attention.self.query.weight True\n",
            "bert.encoder.layer.8.attention.self.query.bias True\n",
            "bert.encoder.layer.8.attention.self.key.weight True\n",
            "bert.encoder.layer.8.attention.self.key.bias True\n",
            "bert.encoder.layer.8.attention.self.value.weight True\n",
            "bert.encoder.layer.8.attention.self.value.bias True\n",
            "bert.encoder.layer.8.attention.output.dense.weight True\n",
            "bert.encoder.layer.8.attention.output.dense.bias True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.8.intermediate.dense.weight True\n",
            "bert.encoder.layer.8.intermediate.dense.bias True\n",
            "bert.encoder.layer.8.output.dense.weight True\n",
            "bert.encoder.layer.8.output.dense.bias True\n",
            "bert.encoder.layer.8.output.LayerNorm.weight True\n",
            "bert.encoder.layer.8.output.LayerNorm.bias True\n",
            "bert.encoder.layer.9.attention.self.query.weight True\n",
            "bert.encoder.layer.9.attention.self.query.bias True\n",
            "bert.encoder.layer.9.attention.self.key.weight True\n",
            "bert.encoder.layer.9.attention.self.key.bias True\n",
            "bert.encoder.layer.9.attention.self.value.weight True\n",
            "bert.encoder.layer.9.attention.self.value.bias True\n",
            "bert.encoder.layer.9.attention.output.dense.weight True\n",
            "bert.encoder.layer.9.attention.output.dense.bias True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.9.intermediate.dense.weight True\n",
            "bert.encoder.layer.9.intermediate.dense.bias True\n",
            "bert.encoder.layer.9.output.dense.weight True\n",
            "bert.encoder.layer.9.output.dense.bias True\n",
            "bert.encoder.layer.9.output.LayerNorm.weight True\n",
            "bert.encoder.layer.9.output.LayerNorm.bias True\n",
            "bert.encoder.layer.10.attention.self.query.weight True\n",
            "bert.encoder.layer.10.attention.self.query.bias True\n",
            "bert.encoder.layer.10.attention.self.key.weight True\n",
            "bert.encoder.layer.10.attention.self.key.bias True\n",
            "bert.encoder.layer.10.attention.self.value.weight True\n",
            "bert.encoder.layer.10.attention.self.value.bias True\n",
            "bert.encoder.layer.10.attention.output.dense.weight True\n",
            "bert.encoder.layer.10.attention.output.dense.bias True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.10.intermediate.dense.weight True\n",
            "bert.encoder.layer.10.intermediate.dense.bias True\n",
            "bert.encoder.layer.10.output.dense.weight True\n",
            "bert.encoder.layer.10.output.dense.bias True\n",
            "bert.encoder.layer.10.output.LayerNorm.weight True\n",
            "bert.encoder.layer.10.output.LayerNorm.bias True\n",
            "bert.encoder.layer.11.attention.self.query.weight True\n",
            "bert.encoder.layer.11.attention.self.query.bias True\n",
            "bert.encoder.layer.11.attention.self.key.weight True\n",
            "bert.encoder.layer.11.attention.self.key.bias True\n",
            "bert.encoder.layer.11.attention.self.value.weight True\n",
            "bert.encoder.layer.11.attention.self.value.bias True\n",
            "bert.encoder.layer.11.attention.output.dense.weight True\n",
            "bert.encoder.layer.11.attention.output.dense.bias True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.11.intermediate.dense.weight True\n",
            "bert.encoder.layer.11.intermediate.dense.bias True\n",
            "bert.encoder.layer.11.output.dense.weight True\n",
            "bert.encoder.layer.11.output.dense.bias True\n",
            "bert.encoder.layer.11.output.LayerNorm.weight True\n",
            "bert.encoder.layer.11.output.LayerNorm.bias True\n",
            "bert.pooler.dense.weight True\n",
            "bert.pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        }
      ],
      "source": [
        "# print layers\n",
        "for name, param in model.named_parameters():\n",
        "   print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b569296",
      "metadata": {
        "id": "9b569296"
      },
      "outputs": [],
      "source": [
        "# freeze base model parameters\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze base model pooling layers\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    if \"pooler\" in name:\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161d7c59",
      "metadata": {
        "id": "161d7c59",
        "outputId": "4c3656f7-0097-4120-fc92-f2d287b82daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert.embeddings.word_embeddings.weight False\n",
            "bert.embeddings.position_embeddings.weight False\n",
            "bert.embeddings.token_type_embeddings.weight False\n",
            "bert.embeddings.LayerNorm.weight False\n",
            "bert.embeddings.LayerNorm.bias False\n",
            "bert.encoder.layer.0.attention.self.query.weight False\n",
            "bert.encoder.layer.0.attention.self.query.bias False\n",
            "bert.encoder.layer.0.attention.self.key.weight False\n",
            "bert.encoder.layer.0.attention.self.key.bias False\n",
            "bert.encoder.layer.0.attention.self.value.weight False\n",
            "bert.encoder.layer.0.attention.self.value.bias False\n",
            "bert.encoder.layer.0.attention.output.dense.weight False\n",
            "bert.encoder.layer.0.attention.output.dense.bias False\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.0.intermediate.dense.weight False\n",
            "bert.encoder.layer.0.intermediate.dense.bias False\n",
            "bert.encoder.layer.0.output.dense.weight False\n",
            "bert.encoder.layer.0.output.dense.bias False\n",
            "bert.encoder.layer.0.output.LayerNorm.weight False\n",
            "bert.encoder.layer.0.output.LayerNorm.bias False\n",
            "bert.encoder.layer.1.attention.self.query.weight False\n",
            "bert.encoder.layer.1.attention.self.query.bias False\n",
            "bert.encoder.layer.1.attention.self.key.weight False\n",
            "bert.encoder.layer.1.attention.self.key.bias False\n",
            "bert.encoder.layer.1.attention.self.value.weight False\n",
            "bert.encoder.layer.1.attention.self.value.bias False\n",
            "bert.encoder.layer.1.attention.output.dense.weight False\n",
            "bert.encoder.layer.1.attention.output.dense.bias False\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.1.intermediate.dense.weight False\n",
            "bert.encoder.layer.1.intermediate.dense.bias False\n",
            "bert.encoder.layer.1.output.dense.weight False\n",
            "bert.encoder.layer.1.output.dense.bias False\n",
            "bert.encoder.layer.1.output.LayerNorm.weight False\n",
            "bert.encoder.layer.1.output.LayerNorm.bias False\n",
            "bert.encoder.layer.2.attention.self.query.weight False\n",
            "bert.encoder.layer.2.attention.self.query.bias False\n",
            "bert.encoder.layer.2.attention.self.key.weight False\n",
            "bert.encoder.layer.2.attention.self.key.bias False\n",
            "bert.encoder.layer.2.attention.self.value.weight False\n",
            "bert.encoder.layer.2.attention.self.value.bias False\n",
            "bert.encoder.layer.2.attention.output.dense.weight False\n",
            "bert.encoder.layer.2.attention.output.dense.bias False\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.2.intermediate.dense.weight False\n",
            "bert.encoder.layer.2.intermediate.dense.bias False\n",
            "bert.encoder.layer.2.output.dense.weight False\n",
            "bert.encoder.layer.2.output.dense.bias False\n",
            "bert.encoder.layer.2.output.LayerNorm.weight False\n",
            "bert.encoder.layer.2.output.LayerNorm.bias False\n",
            "bert.encoder.layer.3.attention.self.query.weight False\n",
            "bert.encoder.layer.3.attention.self.query.bias False\n",
            "bert.encoder.layer.3.attention.self.key.weight False\n",
            "bert.encoder.layer.3.attention.self.key.bias False\n",
            "bert.encoder.layer.3.attention.self.value.weight False\n",
            "bert.encoder.layer.3.attention.self.value.bias False\n",
            "bert.encoder.layer.3.attention.output.dense.weight False\n",
            "bert.encoder.layer.3.attention.output.dense.bias False\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.3.intermediate.dense.weight False\n",
            "bert.encoder.layer.3.intermediate.dense.bias False\n",
            "bert.encoder.layer.3.output.dense.weight False\n",
            "bert.encoder.layer.3.output.dense.bias False\n",
            "bert.encoder.layer.3.output.LayerNorm.weight False\n",
            "bert.encoder.layer.3.output.LayerNorm.bias False\n",
            "bert.encoder.layer.4.attention.self.query.weight False\n",
            "bert.encoder.layer.4.attention.self.query.bias False\n",
            "bert.encoder.layer.4.attention.self.key.weight False\n",
            "bert.encoder.layer.4.attention.self.key.bias False\n",
            "bert.encoder.layer.4.attention.self.value.weight False\n",
            "bert.encoder.layer.4.attention.self.value.bias False\n",
            "bert.encoder.layer.4.attention.output.dense.weight False\n",
            "bert.encoder.layer.4.attention.output.dense.bias False\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.4.intermediate.dense.weight False\n",
            "bert.encoder.layer.4.intermediate.dense.bias False\n",
            "bert.encoder.layer.4.output.dense.weight False\n",
            "bert.encoder.layer.4.output.dense.bias False\n",
            "bert.encoder.layer.4.output.LayerNorm.weight False\n",
            "bert.encoder.layer.4.output.LayerNorm.bias False\n",
            "bert.encoder.layer.5.attention.self.query.weight False\n",
            "bert.encoder.layer.5.attention.self.query.bias False\n",
            "bert.encoder.layer.5.attention.self.key.weight False\n",
            "bert.encoder.layer.5.attention.self.key.bias False\n",
            "bert.encoder.layer.5.attention.self.value.weight False\n",
            "bert.encoder.layer.5.attention.self.value.bias False\n",
            "bert.encoder.layer.5.attention.output.dense.weight False\n",
            "bert.encoder.layer.5.attention.output.dense.bias False\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.5.intermediate.dense.weight False\n",
            "bert.encoder.layer.5.intermediate.dense.bias False\n",
            "bert.encoder.layer.5.output.dense.weight False\n",
            "bert.encoder.layer.5.output.dense.bias False\n",
            "bert.encoder.layer.5.output.LayerNorm.weight False\n",
            "bert.encoder.layer.5.output.LayerNorm.bias False\n",
            "bert.encoder.layer.6.attention.self.query.weight False\n",
            "bert.encoder.layer.6.attention.self.query.bias False\n",
            "bert.encoder.layer.6.attention.self.key.weight False\n",
            "bert.encoder.layer.6.attention.self.key.bias False\n",
            "bert.encoder.layer.6.attention.self.value.weight False\n",
            "bert.encoder.layer.6.attention.self.value.bias False\n",
            "bert.encoder.layer.6.attention.output.dense.weight False\n",
            "bert.encoder.layer.6.attention.output.dense.bias False\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.6.intermediate.dense.weight False\n",
            "bert.encoder.layer.6.intermediate.dense.bias False\n",
            "bert.encoder.layer.6.output.dense.weight False\n",
            "bert.encoder.layer.6.output.dense.bias False\n",
            "bert.encoder.layer.6.output.LayerNorm.weight False\n",
            "bert.encoder.layer.6.output.LayerNorm.bias False\n",
            "bert.encoder.layer.7.attention.self.query.weight False\n",
            "bert.encoder.layer.7.attention.self.query.bias False\n",
            "bert.encoder.layer.7.attention.self.key.weight False\n",
            "bert.encoder.layer.7.attention.self.key.bias False\n",
            "bert.encoder.layer.7.attention.self.value.weight False\n",
            "bert.encoder.layer.7.attention.self.value.bias False\n",
            "bert.encoder.layer.7.attention.output.dense.weight False\n",
            "bert.encoder.layer.7.attention.output.dense.bias False\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.7.intermediate.dense.weight False\n",
            "bert.encoder.layer.7.intermediate.dense.bias False\n",
            "bert.encoder.layer.7.output.dense.weight False\n",
            "bert.encoder.layer.7.output.dense.bias False\n",
            "bert.encoder.layer.7.output.LayerNorm.weight False\n",
            "bert.encoder.layer.7.output.LayerNorm.bias False\n",
            "bert.encoder.layer.8.attention.self.query.weight False\n",
            "bert.encoder.layer.8.attention.self.query.bias False\n",
            "bert.encoder.layer.8.attention.self.key.weight False\n",
            "bert.encoder.layer.8.attention.self.key.bias False\n",
            "bert.encoder.layer.8.attention.self.value.weight False\n",
            "bert.encoder.layer.8.attention.self.value.bias False\n",
            "bert.encoder.layer.8.attention.output.dense.weight False\n",
            "bert.encoder.layer.8.attention.output.dense.bias False\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.8.intermediate.dense.weight False\n",
            "bert.encoder.layer.8.intermediate.dense.bias False\n",
            "bert.encoder.layer.8.output.dense.weight False\n",
            "bert.encoder.layer.8.output.dense.bias False\n",
            "bert.encoder.layer.8.output.LayerNorm.weight False\n",
            "bert.encoder.layer.8.output.LayerNorm.bias False\n",
            "bert.encoder.layer.9.attention.self.query.weight False\n",
            "bert.encoder.layer.9.attention.self.query.bias False\n",
            "bert.encoder.layer.9.attention.self.key.weight False\n",
            "bert.encoder.layer.9.attention.self.key.bias False\n",
            "bert.encoder.layer.9.attention.self.value.weight False\n",
            "bert.encoder.layer.9.attention.self.value.bias False\n",
            "bert.encoder.layer.9.attention.output.dense.weight False\n",
            "bert.encoder.layer.9.attention.output.dense.bias False\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.9.intermediate.dense.weight False\n",
            "bert.encoder.layer.9.intermediate.dense.bias False\n",
            "bert.encoder.layer.9.output.dense.weight False\n",
            "bert.encoder.layer.9.output.dense.bias False\n",
            "bert.encoder.layer.9.output.LayerNorm.weight False\n",
            "bert.encoder.layer.9.output.LayerNorm.bias False\n",
            "bert.encoder.layer.10.attention.self.query.weight False\n",
            "bert.encoder.layer.10.attention.self.query.bias False\n",
            "bert.encoder.layer.10.attention.self.key.weight False\n",
            "bert.encoder.layer.10.attention.self.key.bias False\n",
            "bert.encoder.layer.10.attention.self.value.weight False\n",
            "bert.encoder.layer.10.attention.self.value.bias False\n",
            "bert.encoder.layer.10.attention.output.dense.weight False\n",
            "bert.encoder.layer.10.attention.output.dense.bias False\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.10.intermediate.dense.weight False\n",
            "bert.encoder.layer.10.intermediate.dense.bias False\n",
            "bert.encoder.layer.10.output.dense.weight False\n",
            "bert.encoder.layer.10.output.dense.bias False\n",
            "bert.encoder.layer.10.output.LayerNorm.weight False\n",
            "bert.encoder.layer.10.output.LayerNorm.bias False\n",
            "bert.encoder.layer.11.attention.self.query.weight False\n",
            "bert.encoder.layer.11.attention.self.query.bias False\n",
            "bert.encoder.layer.11.attention.self.key.weight False\n",
            "bert.encoder.layer.11.attention.self.key.bias False\n",
            "bert.encoder.layer.11.attention.self.value.weight False\n",
            "bert.encoder.layer.11.attention.self.value.bias False\n",
            "bert.encoder.layer.11.attention.output.dense.weight False\n",
            "bert.encoder.layer.11.attention.output.dense.bias False\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.11.intermediate.dense.weight False\n",
            "bert.encoder.layer.11.intermediate.dense.bias False\n",
            "bert.encoder.layer.11.output.dense.weight False\n",
            "bert.encoder.layer.11.output.dense.bias False\n",
            "bert.encoder.layer.11.output.LayerNorm.weight False\n",
            "bert.encoder.layer.11.output.LayerNorm.bias False\n",
            "bert.pooler.dense.weight True\n",
            "bert.pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        }
      ],
      "source": [
        "# print layers\n",
        "for name, param in model.named_parameters():\n",
        "   print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2e421a",
      "metadata": {
        "id": "5d2e421a"
      },
      "source": [
        "#### Preprocess text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0dc1e1",
      "metadata": {
        "id": "cf0dc1e1"
      },
      "outputs": [],
      "source": [
        "# define text preprocessing\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42616c7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "68bda069bb5f4ee599b1470261a70e51",
            "ba15c888c81d433f98afd932f340cbe3",
            "b885e0c7a47d484fa9f9d173ae5936ea"
          ]
        },
        "id": "b42616c7",
        "outputId": "d750631d-524e-43af-9f5a-b9b3834dac5d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68bda069bb5f4ee599b1470261a70e51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba15c888c81d433f98afd932f340cbe3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b885e0c7a47d484fa9f9d173ae5936ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# tokenize all datasetse\n",
        "tokenized_data = dataset_dict.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a04a18",
      "metadata": {
        "id": "04a04a18"
      },
      "outputs": [],
      "source": [
        "# create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e2ce8c7",
      "metadata": {
        "id": "7e2ce8c7"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b12c6d",
      "metadata": {
        "id": "16b12c6d"
      },
      "outputs": [],
      "source": [
        "# load metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "auc_score = evaluate.load(\"roc_auc\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # get predictions\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # apply softmax to get probabilities\n",
        "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
        "    # use probabilities of the positive class for ROC AUC\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    # compute auc\n",
        "    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n",
        "\n",
        "    # predict most probable class\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    # compute accuracy\n",
        "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n",
        "\n",
        "    return {\"Accuracy\": acc, \"AUC\": auc}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68613386",
      "metadata": {
        "id": "68613386"
      },
      "source": [
        "#### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579f1230",
      "metadata": {
        "id": "579f1230"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert-phishing-classifier_teacher\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4df3d5",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9a07be7bbba84677be08410608757647",
            "23c4d13738c5410b993f6f493fb5bfc8",
            "06611a26027f4241a70ee1a7ab0ec71a",
            "bf71e0513375417482a0f352f301ee59",
            "30ccc9bcd2e34d278e43d0f0aeb38973",
            "e51e4341d97b4a9a8ade7ab151ea6993",
            "75a23db73c004b8ebaa2b222e0f79557",
            "76df59d18f3242db8447201e50519046",
            "359d2081a737409191ddfe33a266f500",
            "d7832fb128f040c4881747a05acc31c1",
            "e831e5bf2c60464fa41539c0199b534e"
          ]
        },
        "id": "7c4df3d5",
        "outputId": "086d82b4-c808-4d4e-cd7c-137c39896b83"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a07be7bbba84677be08410608757647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2630 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4916, 'grad_norm': 1.5799760818481445, 'learning_rate': 0.00018, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23c4d13738c5410b993f6f493fb5bfc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.42275097966194153, 'eval_Accuracy': 0.784, 'eval_AUC': 0.915, 'eval_runtime': 14.4369, 'eval_samples_per_second': 31.17, 'eval_steps_per_second': 3.948, 'epoch': 1.0}\n",
            "{'loss': 0.3894, 'grad_norm': 2.128021478652954, 'learning_rate': 0.00016, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06611a26027f4241a70ee1a7ab0ec71a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3585848808288574, 'eval_Accuracy': 0.818, 'eval_AUC': 0.932, 'eval_runtime': 6.0872, 'eval_samples_per_second': 73.925, 'eval_steps_per_second': 9.364, 'epoch': 2.0}\n",
            "{'loss': 0.3837, 'grad_norm': 1.0340383052825928, 'learning_rate': 0.00014, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf71e0513375417482a0f352f301ee59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.31439465284347534, 'eval_Accuracy': 0.86, 'eval_AUC': 0.939, 'eval_runtime': 6.0441, 'eval_samples_per_second': 74.452, 'eval_steps_per_second': 9.431, 'epoch': 3.0}\n",
            "{'loss': 0.3574, 'grad_norm': 1.5584102869033813, 'learning_rate': 0.00012, 'epoch': 4.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30ccc9bcd2e34d278e43d0f0aeb38973",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.44938138127326965, 'eval_Accuracy': 0.807, 'eval_AUC': 0.942, 'eval_runtime': 5.8199, 'eval_samples_per_second': 77.32, 'eval_steps_per_second': 9.794, 'epoch': 4.0}\n",
            "{'loss': 0.3517, 'grad_norm': 3.645578622817993, 'learning_rate': 0.0001, 'epoch': 5.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e51e4341d97b4a9a8ade7ab151ea6993",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.32867559790611267, 'eval_Accuracy': 0.86, 'eval_AUC': 0.947, 'eval_runtime': 6.0139, 'eval_samples_per_second': 74.826, 'eval_steps_per_second': 9.478, 'epoch': 5.0}\n",
            "{'loss': 0.3518, 'grad_norm': 2.798996925354004, 'learning_rate': 8e-05, 'epoch': 6.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75a23db73c004b8ebaa2b222e0f79557",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.30418822169303894, 'eval_Accuracy': 0.871, 'eval_AUC': 0.949, 'eval_runtime': 6.0775, 'eval_samples_per_second': 74.044, 'eval_steps_per_second': 9.379, 'epoch': 6.0}\n",
            "{'loss': 0.3185, 'grad_norm': 2.2050981521606445, 'learning_rate': 6e-05, 'epoch': 7.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76df59d18f3242db8447201e50519046",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2899710536003113, 'eval_Accuracy': 0.862, 'eval_AUC': 0.949, 'eval_runtime': 6.0393, 'eval_samples_per_second': 74.512, 'eval_steps_per_second': 9.438, 'epoch': 7.0}\n",
            "{'loss': 0.3267, 'grad_norm': 4.135854721069336, 'learning_rate': 4e-05, 'epoch': 8.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "359d2081a737409191ddfe33a266f500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2957993149757385, 'eval_Accuracy': 0.876, 'eval_AUC': 0.95, 'eval_runtime': 6.0635, 'eval_samples_per_second': 74.214, 'eval_steps_per_second': 9.4, 'epoch': 8.0}\n",
            "{'loss': 0.3153, 'grad_norm': 0.3449840545654297, 'learning_rate': 2e-05, 'epoch': 9.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7832fb128f040c4881747a05acc31c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2881464660167694, 'eval_Accuracy': 0.867, 'eval_AUC': 0.951, 'eval_runtime': 6.103, 'eval_samples_per_second': 73.734, 'eval_steps_per_second': 9.34, 'epoch': 9.0}\n",
            "{'loss': 0.3061, 'grad_norm': 4.887284278869629, 'learning_rate': 0.0, 'epoch': 10.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e831e5bf2c60464fa41539c0199b534e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2963337004184723, 'eval_Accuracy': 0.873, 'eval_AUC': 0.951, 'eval_runtime': 6.2822, 'eval_samples_per_second': 71.631, 'eval_steps_per_second': 9.073, 'epoch': 10.0}\n",
            "{'train_runtime': 418.2572, 'train_samples_per_second': 50.208, 'train_steps_per_second': 6.288, 'train_loss': 0.35921568199708886, 'epoch': 10.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2630, training_loss=0.35921568199708886, metrics={'train_runtime': 418.2572, 'train_samples_per_second': 50.208, 'train_steps_per_second': 6.288, 'total_flos': 706603239165360.0, 'train_loss': 0.35921568199708886, 'epoch': 10.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6b12cc",
      "metadata": {
        "id": "cf6b12cc"
      },
      "source": [
        "### Apply Model to Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1f2710",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "102b3214fe3d4e7aa5bda3b559bee013"
          ]
        },
        "id": "ba1f2710",
        "outputId": "f64d92c4-dc4f-4689-bdb6-7b821308420b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "102b3214fe3d4e7aa5bda3b559bee013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/57 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Accuracy': 0.889, 'AUC': 0.946}\n"
          ]
        }
      ],
      "source": [
        "# apply model to validation dataset\n",
        "predictions = trainer.predict(tokenized_data[\"validation\"])\n",
        "\n",
        "# Extract the logits and labels from the predictions object\n",
        "logits = predictions.predictions\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Use your compute_metrics function\n",
        "metrics = compute_metrics((logits, labels))\n",
        "print(metrics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}